讲真，这个竞赛最后能拿到第七名我觉得不可思议...给我的感受就是一句话，garbage in, garbage out...

个人认为这个过程没多少参考价值，包括代码，所以我也不是很想传代码，毕竟还不如去看原版开源，这个没啥适用性，听完下面的过程就知道为啥这么说了。

首先，这个竞赛的数据很奇葩，分布不均，训练集基本都是专家模板构造的一堆数据，可以说经过预处理之后都可以完美匹配数据库那种，而测试集存在一部分的线上数据，包括各类错别字同音字，识别起来简直是一种痛苦，所以加了一定量的后处理；

其次，缺乏算力，之前参加的比赛最好的也是20多名，因为缺乏算力，只有一张2080ti，还得等到下班了用，训练时间又很久，几乎一天才能训练一个模型，可以用调试时间太短了，几乎没办法在模型层面做更多的操作，只能在数据层面做各种处理；

最后，背景讲完，开始讲一下思路：

1. 拿到题目发现有足够的参考模型，Wikisql和Spider两个经典的Text2sql数据集积攒了不少开源代码，受制于算力，没有选择生成类模型，选择的是HydraNet：https://github.com/lyuqin/HydraNet-WikiSQL ；
2. 搞好了开始硬train一发，效果很一般，考虑到金融相关数据集，所以把底层模型换成了Finbert：https://github.com/valuesimplex/FinBERT ，有一定的改善；
3. 数据分析，剔除专家模板，将原有训练集的8万左右数据压缩到了4万，提升了训练速度，后续发现很多存在于数据库但是没有识别到的，因此针对不同类别做数据增强，随机获取数据库值构造新训练集大约6k条，加入训练，线上指标有一定提升；
4. 数据分析，由于Wikisql数据集和竞赛数据集对于双实体的标注是不一致的，导致代码硬train解码出现异常，总有一个实体解码错误，试探性做了双指针解码，CRF解码，效果都不是很理想，也没那么多时间继续尝试了，选择了对于双实体做两次解码，第二次解码的时候去除第一次的解码结果，同时对于双实体做训练数据增强，拆分成两条单实体加入训练数据，线上数据有一定提升；
5. 后处理，这个其实是提升最高的一部分，根据数据分析，匹配到的文本和对应库中文本不一致导致线上数据较低，所以找了个最简单粗暴的手段，模糊匹配，直接上rapidfuzz对于特定栏位做硬匹配，效果还是很显著，但是也有一些问题，比如有些没有出现在库中的人名基金名被硬匹配走了，也没办法，做了一下tradeoff，最终也就这样了。

所以看到这里也就知道为啥不上代码了，纯数据处理，模型层面还不如去看HydraNet的代码和论文，很感谢有这次机会，玩到了Text2sql相关的知识，甚至还偷了个奖。但是我还是想说一句：咱下次竞赛能不能数据分布更一致一些啊...

22.11.10 update:

更新了代码，真的真的真的不如去看原版，我这一点点创新都没有，满满的都是竞赛数据的过拟合，另外，数据集已经更新，我也在这个版本上面训练了一发，请大家尽管去玩：https://tianchi.aliyun.com/dataset/139273 。

而且发现我之前做的数据集还有错误，不过本着遵循原版的精神，没有做后续优化，有想法的同学可以继续尝试，感谢！

执行流程：

0. 基本环境是python 3.9，cudatoolkit 10.2，cudnn 7.6.2，其他见requirements.txt，append_v5.jsonl生成方式在append_code文件夹，banlist_v2.txt是我使用正则等方式手动过滤生成的；
1. 先把数据放到data文件夹；
2. 执行gen_train_data，构造训练数据；
3. 根据构造的数据，修改conf文件夹下的wikisql.conf内容；
4. 执行train.py，时间可能很久，请等待，我是使用第二个epoch的模型；
5. 根据情况修改inference.py内容，生成结果。


